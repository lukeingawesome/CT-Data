python3 -m torch.distributed.launch --nproc_per_node=4 \
	--use_env training/main.py \
        --enable-deepspeed \
        --grad-checkpointing \
        --name="TOY-CT" \
        --local-loss \
        --save-frequency 4  \
        --zeroshot-frequency 1 \
        --report-to="tensorboard, wandb" \
        --wandb-project-name="TOY-CT" \
        --wandb-notes="TOY-CT-Training" \
        --train-data "./csv/toy_ct.csv" \
        --val-data "./csv/toy_ct.csv" \
        --precision "bf16" \
        --warmup 100 \
        --batch-size=8 \
        --eval-batch-size=8 \
        --tila-loss 1 \
        --log-every-n-steps 1000 \
        --epochs=30 \
        --lr=1e-4 \
        --visual-lr=1e-4 \
        --text-lr=1e-4 \
        --projection-lr=1e-4 \
        --wd=0.05 \
        --visual-wd=0.05 \
        --text-wd=0.05 \
        --ld=1.0 \
        --text-ld=1.01 \
        --visual-ld=1.0 \
        --grad-clip-norm=5.0 \
        --smoothing=0. \
        --model-pth "/data/research/checkpoint-4896/pytorch_model.bin" \
        --workers=4 \
        --seed 4096 \
        --gather-with-grad \
        --force-custom-clip \
        --optimizer="ap_adamw" \
        --zero-stage=1 \
        --dataset-type "ct" \
        --csv-img-key "img_path" \
        --csv-caption-key "findings" \
        --text-separator " [SEP] " \
        --split-column "split" \
        --train-split "train" \
        --val-split "val" \
        --text-base "microsoft/LLM2CLIP-Llama-3.2-1B-Instruct-CC-Finetuned" \

